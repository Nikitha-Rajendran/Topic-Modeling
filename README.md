# Topic-Modeling

In this repository, you will find a project I had done during my undergraduate studies on Topic Modeling in Natural Language Processing which was done as part of a comparative study on various models used for the same. Initially, a Spam-Ham classifier and Disaster Tweet classifier were developed using LSTM models with embeddings. Preprocessing methods like cleaning, tokenization, and GloVe embeddings were employed. A Multi-Class Topic Modeling task on news articles using GRU layers demonstrated good accuracy. Later, a combined dataset with four tweet classes was created, and models like LSTM with GloVe, GRU with Universal Sentence Encoder, and BERT with Latent Dirichlet Allocation were tested. Evaluation metrics included accuracy, precision, recall, and F1 scores, achieving decent results across tasks.


| Problem                         | Repository |
| ------------------------------- | - |
| Spam-Ham Message Classifier(Attempt 1) | [CLICK.](https://github.com/Nikitha-Rajendran/spam-ham-message-classifier-1)  |
| Spam-Ham Message Classifier(Attempt 2) | [CLICK.](https://github.com/Nikitha-Rajendran/spam-ham-2) |
| Spam-Ham Message Classifier(Final) | [CLICK.](https://github.com/Nikitha-Rajendran/spam-ham-final)       |
| Multi-Class Topic Modeling | [CLICK.](https://github.com/Nikitha-Rajendran/multiclass-modeling)   |

Given below is a tabular summary:

|Task|Dataset Details|Preprocessing|Model and Architecture|Training Results|Test Results|
|Spam-Ham Classifier|||||
